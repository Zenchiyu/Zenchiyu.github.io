[
  {
    "path": "posts/2023-08-27-pca-principal-component-analysis/",
    "title": "PCA: Principal Component Analysis (Draft)",
    "description": "Principal Component Analysis, for short PCA, is an *unsupervised learning* method and can be used, depending on specific circumstances, as a *dimensionality reduction technique*.\n\nHowever, PCA per say is an exact decomposition method. One can summarize PCA as the application of the *eigenvalue decomposition* (EVD) on the *data covariance matrix*.",
    "author": [
      {
        "name": "Stéphane Liem Nguyen",
        "url": {}
      }
    ],
    "date": "2023-08-27",
    "categories": [
      "unsupervised learning"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nUnsupervised Learning\r\nDimensionality reduction\r\nEigenvalue Decomposition\r\n\r\nPrincipal Component Analysis\r\nAssumptions\r\nIntuition\r\nRelation with mathematical formulas\r\nEckart-Young Theorem and how many dimensions to keep based on the eigenspectrum ?\r\nConclusion\r\n\r\n\r\nIntroduction\r\n\r\nPrincipal Component Analysis, for short PCA, is an unsupervised learning method and can be used, depending on specific circumstances, as a dimensionality reduction technique. However, PCA per say is an exact decomposition method. One can summarize PCA as the application of the eigenvalue decomposition (EVD) on the data covariance matrix.\r\n\r\nIn this article, you will go through an explanation of what is PCA, both intuitively and slightly formalized using some linear algebra.\r\nUnsupervised Learning\r\nIn the field of Machine Learning, there’s a sub-field called unsupervised learning.\r\nUnsupervised learning methods are used on non-labeled data, e.g. images of cats and dogs but w/o anything stating which class they belong to. These methods can be used for tasks such as clustering, density estimation, visualization and dimensionality reduction.\r\nDimensionality reduction\r\nMachine Learning methods can suffer from large dimensions. For instance, images can be composed of millions of pixels.\r\nThe large number of dimensions can be a burden in terms of computational resources but not only. Intuition from 2 dimensions, 3 dimensions or maybe up to 4 dimensions are hard to extend to thousands of dimensions.\r\nA keyword often used in Machine Learning is the Curse of Dimensionality, we will go more into it in other articles.\r\n\r\nTherefore, dimensionality reduction techniques can be used to shrink that number of dimensions. However, loss of information can occur ! You can think of it as compressing images and decompressing them. Can you retrieve the original image exactly ? There are lossy and lossless compression techniques and they can have different compression ratios.\r\nBecause loss of information can occur, we need to ensure that the dimensionality reduction technique retains useful information for a downstream task, which could be classification.\r\nNote that we’ve only talked about unsupervised learning but dimensionality reduction technique also exist for supervised learning methods, methods that are applied on labeled data.\r\n\r\nEigenvalue Decomposition\r\nPrincipal Component Analysis\r\nAssumptions\r\nIntuition\r\nRelation with mathematical formulas\r\nEckart-Young Theorem and how many dimensions to keep based on the eigenspectrum ?\r\nConclusion\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-08-27T21:00:52+02:00",
    "input_file": "pca-principal-component-analysis.knit.md"
  },
  {
    "path": "posts/2023-08-26-who-am-i/",
    "title": "Who Am I",
    "description": "Introduction",
    "author": [
      {
        "name": "Stéphane Liem Nguyen",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [
      "intro"
    ],
    "contents": "\r\n\r\nContents\r\nCurrent Day\r\nDay 0\r\n\r\nCurrent Day\r\nI’m Stéphane and I’m a computer science student at University of Geneva.\r\nMy academic interests revolve around Machine Learning, more specifically Reinforcement Learning.\r\nMore generally, I’m interested in anything related to decision-making.\r\nBut what ignited that spark of interest ?\r\nDay 0\r\nArtificial Intelligence used to be a foreign concept to me, outside of my interest radar. However, an event at my high school showcasing different scientific disciplines changed that.\r\nThis is in this event, at Collège Claparède, that I discovered the concept of Neural Networks. It was explained to us as a black box, taking an image of a dog and spitting out the class label saying whether it’s a dog or a cat, which led to some frustration.\r\nWhy did he not explain how to construct that black box ? How to construct it ?\r\nThese questions led to the start of my journey.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-08-27T14:24:49+02:00",
    "input_file": {}
  }
]
