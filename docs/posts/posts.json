[
  {
    "path": "posts/2023-10-12-bayesian-inference/",
    "title": "From MLE or MAP to famous loss functions like MSE or Cross Entropy",
    "description": "Have you ever wondered why do we use the Mean Squared Error or Cross Entropy as loss function?",
    "author": [
      {
        "name": "Stéphane Liem Nguyen",
        "url": {}
      }
    ],
    "date": "2023-10-12",
    "categories": [
      "bayesian inference"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nMaximum (Log) Likelihood Estimation vs. Maximum A Posterior\r\nSupervised learning\r\nRegression\r\nClassification\r\n\r\nUnsupervised Learning\r\nDensity estimation/Generative Modelling\r\n\r\n\r\nIntroduction\r\nHave you ever wondered why do we use the Mean Squared Error or Cross Entropy as loss function? In this article, you will discover the relation between Maximum (Log) Likelihood Estimation (MLE), Maximum A Posterior (MAP), and a few commonly used loss functions.\r\nMaximum (Log) Likelihood Estimation vs. Maximum A Posterior\r\nIn Machine Learning, we might intuitively seek the most likely model parameters \\(\\theta\\) given a training set \\(\\mathcal{D}\\). Mathematically speaking, this translates into maximizing the following:\r\n\\[\r\np(\\theta | \\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})}\r\n\\]\r\nor equivalently:\r\n\\[\r\n\\text{posterior} = \\frac{\\text{likelihood} \\cdot \\text{prior}}{\\text{evidence}}\r\n\\]\r\nMaximum A Posterior (MAP)\r\nFinding the parameters that maximize the posterior thus leads to the Maximum A Posterior (MAP)\r\n\\[\r\n\\text{argmax}_\\theta\\,\\, p(\\theta | \\mathcal{D}) = \\text{argmax}_\\theta\\,\\, \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{p(\\mathcal{D})} = \\text{argmax}_\\theta\\,\\, p(\\mathcal{D}|\\theta) p(\\theta)\r\n\\]\r\nSince the logarithm is monotonic, we can also maximize the log posterior. Maximizing the log posterior has advantages like:\r\nChanging products into summations\r\nKilling any exponential term\r\nMaximum Likelihood Estimation (MLE)\r\nIn contrast, finding the parameters that maximize the likelihood thus leads to the Maximum Likelihood Estimation (MLE)\r\n\\[\r\n\\text{argmax}_\\theta\\,\\, p(\\mathcal{D}|\\theta)\r\n\\]\r\nSimilar to MAP we can also maximize the log likelihood.\r\nSupervised learning\r\nApplying MAP in a supervised learning setup amounts to the following:\r\n\\[\r\n\\begin{align}\r\n\\text{argmax}_\\theta p(\\theta|\\mathcal{D}) &= \\text{argmax}_\\theta \\log p(\\theta|\\mathcal{D})\\\\\r\n&=\\text{argmax}_\\theta \\log p(\\mathcal{D}|\\theta) \\cdot p(\\theta),\\,\\text{Bayes rule}\\\\ &= \\text{argmax}_\\theta \\log \\prod_i p((x_i, y_i)|\\theta)\\cdot p(\\theta),\\,\\text{i.i.d}\\\\\r\n&= \\text{argmax}_\\theta \\sum_i \\log\\left(p(y_i|\\theta, x_i)p(x_i|\\theta)\\cdot p(\\theta)\\right),\\,\\text{chain rule}\r\n\\\\\r\n&= \\text{argmax}_\\theta \\sum_i \\log\\left(p(y_i|\\theta, x_i)\\cdot p(\\theta)\\right) + \\log p(x_i),\\,\\text{input indep. of parameters}\r\n\\\\\r\n&= \\text{argmax}_\\theta \\color{red}{\\sum_i \\log\\left(p(y_i|\\theta, x_i)\\right)}+ \\log p(\\theta)\r\n\\end{align}\r\n\\]\r\nwhere the red term is the same as in maximum log likelihood.\r\nRegression\r\nIn regression tasks we might assume the target to be a random variable \\(y_i\\) following a normal distribution centered on the underlying structure \\(f(x_i)\\):\r\n\\[\r\nY_i = f(X_i) + \\epsilon_i,\\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon)\r\n\\]\r\nwhere \\(f\\) is a deterministic function.\r\nEquivalently, we would assume that the i.i.d residuals \\(\\epsilon_i\\) follow a normal distribution of standard deviation \\(\\sigma_\\epsilon\\). Such assumption leads to the Mean Squared Error (MSE)!\r\nMean Squared Error with or without regularization\r\nApplying MAP with such assumptions and a Gaussian prior on the parameters \\(p(\\theta)\\) lead to the MSE with the L2 penalty since the logarithms in the following equation kills the exponential in any normal distribution probability density function formulas:\r\n\r\nApplying MLE with the same assumptions lead to MSE without regularization\r\n\\[\\text{argmax}_\\theta \\color{red}{\\sum_i \\log\\left(p(y_i|\\theta, x_i)\\right)}+ \\log p(\\theta)\\]\r\nA Laplacian prior \\(p(\\theta)\\) leads to the L1 penalty.\r\nClassification\r\nSince the previous assumption doesn’t make any sense in classification tasks, we should not use the MSE loss function in classification tasks. What should we use instead?\r\nFirst recall that we want to maximize the following in MLE:\r\n\\[\r\n\\begin{align}\r\n\\color{red}{\\sum_i \\log\\left(p(y_i|\\theta, x_i)\\right)}\r\n\\end{align}\r\n\\]\r\nMaximizing it is equivalent to doing the following:\r\n\\[\r\n\\begin{align}\r\n\\text{argmax}_\\theta\\,\\,\\color{red}{\\sum_i \\log\\left(p(y_i|\\theta, x_i)\\right)}&=\\text{argmin}_\\theta\\,\\,-\\color{red}{\\sum_i \\log\\left(p(y_i|\\theta, x_i)\\right)}\\\\\r\n&=\\text{argmin}_\\theta\\,\\,\\frac{1}{N}\\color{red}{\\sum_i \\color{black}{-}\\log\\left(p(y_i|\\theta, x_i)\\right)}\\\\\r\n&=\\text{argmin}_\\theta\\,\\,\\frac{1}{N}\\color{red}{\\sum_i \\color{black}{-\\sum_k \\delta_{k, y_i}}\\log\\left(p(k|\\theta, x_i)\\right)}\\\\\r\n\\end{align}\r\n\\]\r\nTherefore, maximizing the log likelihood is equivalent to minimizing the average cross entropy:\r\n\\[\r\n\\begin{align}\r\n\\text{argmin}_\\theta\\,\\,\\frac{1}{N} \\sum_i \\left(-\\sum_k p(k|x_i)\\log\\left(p(k|\\theta, x_i)\\right)\\right)\\\\\r\n\\end{align}\r\n\\]\r\nwhere \\(\\delta_{k, y_i}\\) is the true posterior distribution \\(p(k|x_i)\\). In practice, if we use a softmax to get our probability mass function:\r\n\\[\r\n\\begin{align}\r\np(y_i|x_i, \\theta) &= \\frac{e^{f_\\theta(x_i)_{y_i}}}{\\sum_{k=1}^C e^{f_\\theta(x_i)_{k}}},\\quad y_i \\in \\{1, \\dots, C\\}\r\n\\end{align}\r\n\\]\r\nwhere \\(f_\\theta(x_i)_{k}\\)s are called logits, we can work with a one-hot encoding vector representing \\(p(k|x_i)\\) for each training sample and logits \\(f_\\theta(x_i)_{k}\\)s.\r\n\r\nLogits are what we obtain right before applying the softmax to get probabilities.\r\nUnsupervised Learning\r\nDensity estimation/Generative Modelling\r\nBy applying MLE, we get the empirical cross entropy to minimize\r\n\\[\r\n\\begin{align}\r\n\\text{argmax}_\\theta \\log p(\\mathcal{D}|\\theta) &= \\text{argmax}_\\theta \\log \\prod_i p(x_i|\\theta)\\\\\r\n&= \\text{argmax}_\\theta \\sum_i \\log p(x_i|\\theta)\\\\\r\n&= \\text{argmin}_\\theta -\\frac{1}{N}\\sum_i \\log p(x_i|\\theta)\\\\\r\n&= \\hat{\\mathbb{E}}_{p(x)}\\left[-p_\\theta(x)\\right]\r\n\\end{align}\r\n\\]\r\nwhere \\(p(x)\\) is the true distribution.\r\n\r\n\r\n\r\nFleuret, François. 2023. Deep Learning Course. URL https://fleuret.org/dlc/.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-10-12T22:15:03+02:00",
    "input_file": "bayesian-inference.knit.md"
  },
  {
    "path": "posts/2023-08-27-pca-principal-component-analysis/",
    "title": "PCA: Principal Component Analysis (Draft)",
    "description": "What is Principal Component Analysis ?",
    "author": [
      {
        "name": "Stéphane Liem Nguyen",
        "url": {}
      }
    ],
    "date": "2023-08-27",
    "categories": [
      "unsupervised learning"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nUnsupervised Learning\r\nDimensionality reduction\r\nEigenvalue Decomposition\r\n\r\nPrincipal Component Analysis\r\nAssumptions\r\nIntuition\r\nRelation with mathematical formulas\r\nEckart-Young Theorem and how many dimensions to keep based on the eigenspectrum ?\r\nConclusion\r\n\r\n\r\nIntroduction\r\nPrincipal Component Analysis for short PCA, is an unsupervised learning method that can be used, depending on specific circumstances, as a dimensionality reduction technique. However, PCA is an exact decomposition method. PCA is the eigenvalue decomposition (EVD) of the data covariance matrix.\r\nIn this article, you will see an explanation of what PCA is, intuitively and slightly formalized using some linear algebra.\r\nUnsupervised Learning\r\nThe field of Machine Learning includes a sub-field called unsupervised learning.\r\nUnsupervised learning methods apply to non-labeled data, e.g. images of cats and dogs deprived of class belonging information. These methods are useable for clustering, density estimation, visualization and dimensionality reduction.\r\nDimensionality reduction\r\nMachine Learning methods can suffer from large dimensions. For instance, images can be composed of millions of pixels.\r\nThe large number of dimensions not only can be a burden in terms of computational resources, but intuition from 2, 3 or maybe up to 4 dimensions is hard to extend to thousands of dimensions.\r\nA keyword often used in Machine Learning is the Curse of Dimensionality, which would require further explanation via future articles.\r\n\r\nTherefore, shrinking that number of dimensions requires dimensionality reduction techniques. However, loss of information can occur! You can think of it as compressing images and decompressing them. Can you retrieve the original image from a compressed one without any loss? Lossy and lossless compression techniques exist with different compression ratios.\r\nBecause loss of information can occur, we need to ensure that the dimensionality reduction technique retains useful information for a downstream task, which could be classification.\r\n\r\nUsefulness needs a proper definition\r\nBecause classification is a supervised learning task in which we have labeled data, dimensionality reduction techniques are therefore not fully reserved for unsupervised learning methods.\r\n\r\nEigenvalue Decomposition\r\nPrincipal Component Analysis\r\nAssumptions\r\nPCA assumes the following:\r\ncontinuous features\r\ni.i.d examples coming from a normal distribution so not a multi-modal distribution.\r\nunlabeled data\r\nThe normal distribution assumption comes from the fact that PCA computes a mean vector and data covariance matrix from the dataset.\r\nAlthough PCA can be applied without the assumptions, it’s not recommended. We’ll go back to these assumptions later on.\r\n\r\nIntuition\r\nRelation with mathematical formulas\r\nEckart-Young Theorem and how many dimensions to keep based on the eigenspectrum ?\r\nConclusion\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-08-31T20:09:50+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-08-26-who-am-i/",
    "title": "Who Am I",
    "description": "Introduction",
    "author": [
      {
        "name": "Stéphane Liem Nguyen",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [
      "intro"
    ],
    "contents": "\r\n\r\nContents\r\nCurrent Day\r\nDay 0\r\n\r\nCurrent Day\r\nI’m Stéphane and I’m a computer science student at University of Geneva.\r\nMy academic interests revolve around Machine Learning, more specifically Reinforcement Learning.\r\nMore generally, I’m interested in anything related to decision-making.\r\nBut what ignited that spark of interest ?\r\nDay 0\r\nArtificial Intelligence used to be a foreign concept to me, outside of my interest radar. However, an event at my high school showcasing different scientific disciplines changed that.\r\nThis is in this event, at Collège Claparède, that I discovered the concept of Neural Networks. It was explained to us as a black box, taking an image of a dog and spitting out the class label saying whether it’s a dog or a cat, which led to some frustration.\r\nWhy did he not explain how to construct that black box ? How to construct it ?\r\nThese questions led to the start of my journey.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-10-12T22:09:21+02:00",
    "input_file": {}
  }
]
