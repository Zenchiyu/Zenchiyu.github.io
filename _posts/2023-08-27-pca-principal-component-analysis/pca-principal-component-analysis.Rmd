---
title: "PCA: Principal Component Analysis (Draft)"
description: |
  Principal Component Analysis, for short PCA, is an *unsupervised learning* method and can be used, depending on specific circumstances, as a *dimensionality reduction technique*.
  
  However, PCA per say is an exact decomposition method. One can summarize PCA as the application of the *eigenvalue decomposition* (EVD) of the *data covariance matrix*.
author:
  - first_name: StÃ©phane Liem
    last_name: Nguyen
    affiliation: University of Geneva
    affiliation_url: https://www.unige.ch/dinfo/
date: 2023-08-27
output:
  distill::distill_article:
    toc: true
    self_contained: false
# draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

> Principal Component Analysis, for short PCA, is an *unsupervised learning* method and can be used, depending on specific circumstances, as a *dimensionality reduction technique*. However, PCA per say is an exact decomposition method. One can summarize PCA as the application of the *eigenvalue decomposition* (EVD) of the *data covariance matrix*.


In this article, you will go through an explanation of what is PCA, both intuitively and slightly formalized using some linear algebra.

## Unsupervised Learning

In the field of Machine Learning, there's a sub-field called unsupervised learning.

Unsupervised learning methods are used on non-labeled data, e.g. images of cats and dogs but w/o anything stating which class they belong to. These methods can be used for tasks such as clustering, density estimation, visualization and dimensionality reduction.

## Dimensionality reduction

Machine Learning methods can suffer from large dimensions. For instance, images can be composed of millions of pixels.

The *large number of dimensions* can be a burden in terms of computational resources but not only. Intuition from 2 dimensions, 3 dimensions or maybe up to 4 dimensions are hard to extend to thousands of dimensions.

A keyword often used in Machine Learning is the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">*Curse of Dimensionality*</a>, we will go more into it in other articles.

<!--- to talk about rejection sampling in the future too--->

Therefore, dimensionality reduction techniques can be used to shrink that number of dimensions. However, loss of information can occur ! You can think of it as compressing images and decompressing them. Can you retrieve the original image exactly ? There are lossy and lossless compression techniques and they can have different compression ratios.

Because loss of information can occur, we need to ensure that the dimensionality reduction technique retains useful information for a downstream task, which could be classification.

Note that we've only talked about unsupervised learning but dimensionality reduction technique also exist for supervised learning methods, methods that are applied on labeled data.

<!--- TODO: create some dataset, plot a 2D graph and project data onto one axis to show the loss of information --->


## Eigenvalue Decomposition


# Principal Component Analysis

## Assumptions

## Intuition

## Relation with mathematical formulas

## Eckart-Young Theorem and how many dimensions to keep based on the eigenspectrum ?

## Conclusion

