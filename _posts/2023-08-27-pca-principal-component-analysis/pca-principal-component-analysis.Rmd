---
title: "PCA: Principal Component Analysis (Draft)"
description: |
  What is Principal Component Analysis ?
author:
  - first_name: St√©phane Liem
    last_name: Nguyen
    affiliation: University of Geneva
    affiliation_url: https://www.unige.ch/dinfo/
date: 2023-08-27
categories:
    - unsupervised learning
output:
  distill::distill_article:
    toc: true
    self_contained: false
# draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

Principal Component Analysis for short PCA, is an *unsupervised learning* method that can be used, depending on specific circumstances, as a *dimensionality reduction technique*. However, PCA is an exact decomposition method. PCA is the *eigenvalue decomposition* (EVD) of the *data covariance matrix*.

In this article, you will see an explanation of what PCA is, intuitively and slightly formalized using some linear algebra.

## Unsupervised Learning

The field of Machine Learning includes a sub-field called unsupervised learning.

Unsupervised learning methods apply to non-labeled data, e.g. images of cats and dogs deprived of class belonging information. These methods are useable for clustering, density estimation, visualization and dimensionality reduction.

## Dimensionality reduction

Machine Learning methods can suffer from large dimensions. For instance, images can be composed of millions of pixels.

The *large number of dimensions* not only can be a burden in terms of computational resources, but intuition from 2, 3 or maybe up to 4 dimensions is hard to extend to thousands of dimensions.

A keyword often used in Machine Learning is the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">*Curse of Dimensionality*</a>, which would require further explanation via future articles.

<!--- to talk about rejection sampling in the future too--->

Therefore, shrinking that number of dimensions requires dimensionality reduction techniques. However, loss of information can occur! You can think of it as compressing images and decompressing them. Can you retrieve the original image from a compressed one without any loss? Lossy and lossless compression techniques exist with different compression ratios.

Because loss of information can occur, we need to ensure that the dimensionality reduction technique retains *useful* information for a downstream task, which could be classification.

<aside>Usefulness needs a proper definition</aside>

Because classification is a supervised learning task in which we have labeled data, dimensionality reduction techniques are therefore not fully reserved for unsupervised learning methods.

<!--- TODO: create some dataset, plot a 2D graph and project data onto one axis to show the loss of information --->


## Eigenvalue Decomposition


# Principal Component Analysis

## Assumptions

PCA assumes the following:

- continuous features
- i.i.d examples coming from a normal distribution so not a multi-modal distribution.
- unlabeled data

The normal distribution assumption comes from the fact that PCA computes a mean vector and data covariance matrix from the dataset.

Although PCA can be applied without the assumptions, it's not recommended. We'll go back to these assumptions later on.

<!--- normalizing input features can help PCA --->

## Intuition

## Relation with mathematical formulas

## Eckart-Young Theorem and how many dimensions to keep based on the eigenspectrum ?

## Conclusion

